#!/usr/bin/env python3
"""
HyperStyleç¼–ç å™¨
å°†FFHQå¯¹é½çš„äººè„¸å›¾åƒç¼–ç åˆ°StyleGAN2æ½œåœ¨ç©ºé—´
"""

import torch
import torch.nn as nn
import torchvision.transforms as transforms
import numpy as np
import cv2
from PIL import Image
import os
import sys
from typing import Union, Optional

class HyperStyleEncoder:
    def __init__(self, 
                 model_path: str,
                 device: str = 'auto'):
        """
        åˆå§‹åŒ–HyperStyleç¼–ç å™¨
        
        Args:
            model_path: HyperStyleæ¨¡å‹æ–‡ä»¶è·¯å¾„
            device: è®¡ç®—è®¾å¤‡ ('cuda', 'cpu', 'auto')
        """
        # è®¾å¤‡è®¾ç½®
        if device == 'auto':
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = torch.device(device)
            
        print(f"ğŸ–¥ï¸  HyperStyleä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"HyperStyleæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        
        # w_encoderå›ºå®šè·¯å¾„
        script_dir = os.path.dirname(os.path.abspath(__file__))
        project_root = os.path.dirname(script_dir)
        self.w_encoder_path = os.path.join(project_root, 'models', 'encoders', 'faces_w_encoder.pt')
        
        if not os.path.exists(self.w_encoder_path):
            raise FileNotFoundError(f"Wç¼–ç å™¨æ–‡ä»¶ä¸å­˜åœ¨: {self.w_encoder_path}")
        
        # è®¾ç½®HyperStyleç¯å¢ƒ
        self._setup_hyperstyle_env()
        
        # åŠ è½½æ¨¡å‹
        self.model, self.opts = self.load_model_official(model_path)
        
        # FFHQäººè„¸é¢„å¤„ç†ï¼ˆå®˜æ–¹æ ‡å‡†ï¼‰
        self.transform = transforms.Compose([
            transforms.Resize((256, 256)),
            transforms.ToTensor(),
            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
        ])
        
        print("âœ… HyperStyleç¼–ç å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _setup_hyperstyle_env(self):
        """è®¾ç½®HyperStyleç¯å¢ƒ"""
        # æ·»åŠ hyperstyleè·¯å¾„
        script_dir = os.path.dirname(os.path.abspath(__file__))
        hyperstyle_path = os.path.join(script_dir, 'hyperstyle')
        
        if not os.path.exists(hyperstyle_path):
            raise FileNotFoundError(f"HyperStyleç›®å½•ä¸å­˜åœ¨: {hyperstyle_path}")
        
        if hyperstyle_path not in sys.path:
            sys.path.insert(0, hyperstyle_path)
    
    def load_model_official(self, model_path: str):
        """ä½¿ç”¨å®˜æ–¹æ–¹æ³•åŠ è½½æ¨¡å‹"""
        print(f"ğŸ“¥ åŠ è½½HyperStyleæ¨¡å‹: {os.path.basename(model_path)}")
        
        # å¯¼å…¥å®˜æ–¹çš„load_modelå‡½æ•°
        from utils.model_utils import load_model
        
        # ä½¿ç”¨å®˜æ–¹åŠ è½½æ–¹å¼
        update_opts = {"w_encoder_checkpoint_path": self.w_encoder_path}
        net, opts = load_model(model_path, update_opts=update_opts)
        
        # ç§»åŠ¨åˆ°è®¾å¤‡
        net = net.to(self.device)
        net.eval()
        
        print("âœ… HyperStyleæ¨¡å‹åŠ è½½å®Œæˆ")
        return net, opts
    
    def preprocess_image(self, image_input: Union[str, np.ndarray, Image.Image]) -> torch.Tensor:
        """é¢„å¤„ç†å›¾åƒä¸ºHyperStyleæ ‡å‡†è¾“å…¥"""
        # è½¬æ¢ä¸ºPILå›¾åƒ
        if isinstance(image_input, str):
            pil_image = Image.open(image_input).convert('RGB')
        elif isinstance(image_input, np.ndarray):
            rgb_image = cv2.cvtColor(image_input, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(rgb_image)
        elif isinstance(image_input, Image.Image):
            pil_image = image_input.convert('RGB')
        else:
            raise TypeError(f"ä¸æ”¯æŒçš„è¾“å…¥ç±»å‹: {type(image_input)}")
        
        # åº”ç”¨å˜æ¢
        tensor = self.transform(pil_image)
        tensor = tensor.unsqueeze(0)
        
        return tensor.to(self.device)
    
    def encode_image(self, image_input: Union[str, np.ndarray, Image.Image]) -> torch.Tensor:
        """ç¼–ç å›¾åƒåˆ°StyleGAN2æ½œåœ¨ç©ºé—´"""
        try:
            # é¢„å¤„ç†å›¾åƒ
            input_tensor = self.preprocess_image(image_input)
            print(f"ğŸ” è¾“å…¥å¼ é‡å½¢çŠ¶: {input_tensor.shape}")
            
            # ä½¿ç”¨å®˜æ–¹æ¨ç†
            from utils.inference_utils import run_inversion
            
            with torch.no_grad():
                # è®¾ç½®æ¨ç†å‚æ•°
                self.opts.resize_outputs = False
                if not hasattr(self.opts, 'n_iters_per_batch'):
                    self.opts.n_iters_per_batch = 5
                
                print(f"ğŸ”„ å¼€å§‹æ¨ç†ï¼Œè¿­ä»£æ¬¡æ•°: {self.opts.n_iters_per_batch}")
                
                result_batch, result_latents, _ = run_inversion(
                    input_tensor, 
                    self.model, 
                    self.opts,
                    return_intermediate_results=True
                )
                
                print(f"ğŸ” æ¨ç†ç»“æœç±»å‹: result_batch={type(result_batch)}, result_latents={type(result_latents)}")
                
                # æŒ‰ç…§å®˜æ–¹ä»£ç å¤„ç†
                if isinstance(result_batch, dict) and isinstance(result_latents, dict):
                    print(f"ğŸ” result_batché”®: {list(result_batch.keys())}")
                    print(f"ğŸ” result_latentsé”®: {list(result_latents.keys())}")
                    
                    # result_batch[0] æ˜¯é‡å»ºçš„å›¾åƒå¼ é‡ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰
                    # result_latents[0] æ˜¯æ½œåœ¨å‘é‡ï¼ˆç”¨äºç¼–è¾‘ï¼‰
                    if 0 in result_latents:
                        latent_codes = result_latents[0]  # æ½œåœ¨å‘é‡
                        print(f"ğŸ” latent_codesç±»å‹: {type(latent_codes)}")
                        
                        if isinstance(latent_codes, list) and len(latent_codes) > 0:
                            # è·å–æœ€ç»ˆçš„æ½œåœ¨å‘é‡ï¼ˆæœ€åä¸€æ¬¡è¿­ä»£ï¼‰
                            final_latent = latent_codes[-1]
                            print(f"âœ… è·å¾—æ½œåœ¨å‘é‡ï¼Œå½¢çŠ¶: {final_latent.shape}")
                            
                            # ç¡®ä¿è¿”å›torchå¼ é‡
                            if isinstance(final_latent, np.ndarray):
                                final_latent = torch.from_numpy(final_latent).to(self.device)
                                print(f"ğŸ”„ è½¬æ¢numpyåˆ°torchå¼ é‡: {final_latent.shape}")
                            elif not isinstance(final_latent, torch.Tensor):
                                raise RuntimeError(f"æœªçŸ¥çš„æ½œåœ¨å‘é‡ç±»å‹: {type(final_latent)}")
                            
                            return final_latent
                        else:
                            raise RuntimeError(f"latent_codesæ ¼å¼é”™è¯¯: {type(latent_codes)}")
                    else:
                        raise RuntimeError("result_latentsä¸­æ²¡æœ‰æ‰¾åˆ°æ½œåœ¨å‘é‡")
                else:
                    raise RuntimeError(f"ç»“æœæ ¼å¼é”™è¯¯: result_batch={type(result_batch)}, result_latents={type(result_latents)}")
            
        except Exception as e:
            print(f"âŒ å›¾åƒç¼–ç å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            raise
    
    def encode_batch(self, image_list: list, batch_size: int = 4) -> torch.Tensor:
        """æ‰¹é‡ç¼–ç å›¾åƒ"""
        all_latents = []
        
        for i in range(0, len(image_list), batch_size):
            batch_images = image_list[i:i + batch_size]
            print(f"ğŸ”„ ç¼–ç æ‰¹æ¬¡ {i//batch_size + 1}/{(len(image_list)-1)//batch_size + 1}")
            
            for image in batch_images:
                latent = self.encode_image(image)
                all_latents.append(latent)
        
        return torch.cat(all_latents, dim=0)
    
    def save_latent(self, latent: torch.Tensor, save_path: str):
        """ä¿å­˜æ½œåœ¨å‘é‡åˆ°æ–‡ä»¶"""
        # è·å–ç›®å½•è·¯å¾„
        save_dir = os.path.dirname(save_path)
        
        # å¦‚æœç›®å½•ä¸ä¸ºç©ºï¼Œåˆ›å»ºç›®å½•
        if save_dir and save_dir != '':
            os.makedirs(save_dir, exist_ok=True)
        
        save_data = {
            'latent': latent.cpu(),
            'shape': list(latent.shape),
            'device': str(latent.device),
            'dtype': str(latent.dtype)
        }
        
        torch.save(save_data, save_path)
    
    def load_latent(self, load_path: str) -> torch.Tensor:
        """ä»æ–‡ä»¶åŠ è½½æ½œåœ¨å‘é‡"""
        save_data = torch.load(load_path, map_location=self.device, weights_only=False)
        
        if isinstance(save_data, dict) and 'latent' in save_data:
            latent = save_data['latent'].to(self.device)
        else:
            latent = save_data.to(self.device)
        
        return latent
    
    def get_latent_stats(self, latent: torch.Tensor) -> dict:
        """åˆ†ææ½œåœ¨å‘é‡ç»Ÿè®¡ä¿¡æ¯"""
        latent_cpu = latent.cpu()
        
        return {
            'shape': list(latent.shape),
            'mean': float(torch.mean(latent_cpu)),
            'std': float(torch.std(latent_cpu)),
            'min': float(torch.min(latent_cpu)),
            'max': float(torch.max(latent_cpu)),
            'norm': float(torch.norm(latent_cpu)),
            'device': str(latent.device),
            'dtype': str(latent.dtype)
        }
    
    def encode_and_save(self, image_path: str, output_path: str) -> bool:
        """ç¼–ç å›¾åƒå¹¶ç›´æ¥ä¿å­˜"""
        try:
            # æ£€æŸ¥è¾“å‡ºè·¯å¾„
            if not output_path or output_path.strip() == '':
                raise ValueError("è¾“å‡ºè·¯å¾„ä¸èƒ½ä¸ºç©º")
            
            latent = self.encode_image(image_path)
            self.save_latent(latent, output_path)
            
            return True
        except Exception as e:
            print(f"âŒ ç¼–ç ä¿å­˜å¤±è´¥: {e}")
            return False
    
    def batch_encode_directory(self, input_dir: str, output_dir: str) -> list:
        """æ‰¹é‡ç¼–ç ç›®å½•ä¸­çš„å›¾åƒ"""
        os.makedirs(output_dir, exist_ok=True)
        
        extensions = ['.jpg', '.jpeg', '.png', '.bmp']
        image_files = []
        
        for ext in extensions:
            image_files.extend([
                f for f in os.listdir(input_dir) 
                if f.lower().endswith(ext.lower())
            ])
        
        success_paths = []
        
        for i, filename in enumerate(image_files):
            input_path = os.path.join(input_dir, filename)
            name, _ = os.path.splitext(filename)
            output_path = os.path.join(output_dir, f"{name}_latent.pt")
            
            if self.encode_and_save(input_path, output_path):
                success_paths.append(output_path)
        
        return success_paths

def main():
    """æµ‹è¯•å‡½æ•°"""
    if len(sys.argv) < 4:
        print("ç”¨æ³•:")
        print("  å•å¼ ç¼–ç : python hyperstyle_encoder.py hyperstyle_ffhq.pt input.jpg output.pt")
        print("  æ‰¹é‡ç¼–ç : python hyperstyle_encoder.py hyperstyle_ffhq.pt input_dir/ output_dir/ --batch")
        return
    
    model_path = sys.argv[1]
    input_path = sys.argv[2]
    output_path = sys.argv[3]
    batch_mode = len(sys.argv) > 4 and sys.argv[4] == '--batch'
    
    try:
        encoder = HyperStyleEncoder(model_path)
        
        if batch_mode:
            success_paths = encoder.batch_encode_directory(input_path, output_path)
        else:
            if encoder.encode_and_save(input_path, output_path):
                print("âœ… ç¼–ç å®Œæˆ!")
            else:
                print("âŒ ç¼–ç å¤±è´¥!")
        
    except Exception as e:
        print(f"âŒ å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
