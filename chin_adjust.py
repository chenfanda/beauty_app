#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆä¸‹å·´è°ƒæ•´è„šæœ¬ - åœ¨å…³é”®ç©ºç™½åŒºåŸŸå¢åŠ åè°ƒæ§åˆ¶ç‚¹
è§£å†³æ•´ä½“é¢éƒ¨åè°ƒæ€§é—®é¢˜ï¼Œæ¶ˆé™¤è¾¹ç¼˜åƒç´ æ¼‚ç§»
åŸºäºåŸæœ‰5å±‚æ¶æ„ï¼Œç²¾å‡†æ·»åŠ å…³é”®è¿‡æ¸¡æ§åˆ¶ç‚¹
"""

import cv2
import numpy as np
import mediapipe as mp
import argparse
import os
from typing import List, Tuple, Optional

class OptimizedChin3DAdjustment:
    def __init__(self,face_mesh=None):
        self.mp_face_mesh = mp.solutions.face_mesh
        if not face_mesh:
        # MediaPipeè®¾ç½®
            self.face_mesh = self.mp_face_mesh.FaceMesh(
                static_image_mode=True,
                max_num_faces=1,
                refine_landmarks=True,
                min_detection_confidence=0.8,
                min_tracking_confidence=0.8
            )
        else:
            self.face_mesh = face_mesh
        
        # ç›¸æœºå‚æ•°
        self.camera_matrix = None
        self.dist_coeffs = None
        
        # ğŸ¯ åŸæœ‰5å±‚æ¶æ„ï¼ˆä¿æŒä¸å˜ï¼‰
        # ç¬¬1å±‚ï¼šç¡¬é”šç‚¹ï¼ˆç»å¯¹ç¨³å®šï¼‰- è„¸å‹åŸºå‡†æ¡†æ¶
        self.HARD_ANCHORS = [10, 338, 297]  # é¢å¤´ä¸­ç‚¹å’Œä¸¤ä¾§é¢§éª¨ç‚¹
        
        # ç¬¬2å±‚ï¼šå¼±é”šç‚¹ï¼ˆå¾®è·Ÿéšç¨³å®šï¼‰- ä¸­è„¸åŒºåŸŸé˜²æŠ¤
        self.WEAK_ANCHORS = [117, 123, 147, 346, 352, 376]  # è„¸é¢ŠåŒºåŸŸç¨³å®šç‚¹
        
        # ç¬¬3å±‚ï¼šåŠ¨æ€é”šç‚¹ï¼ˆä»»åŠ¡è‡ªé€‚åº”ï¼‰- æ ¹æ®å˜å½¢ç±»å‹å’Œå§¿æ€åŠ¨æ€å¯ç”¨
        self.DYNAMIC_ANCHORS = [234, 454, 93, 323]  # ä¸‹é¢Œèµ·å§‹ç‚¹
        
        # ç¬¬4å±‚ï¼šè¿‡æ¸¡å±‚ï¼ˆæ¸å˜è·Ÿéšï¼‰- é”šç‚¹åˆ°å˜å½¢ç‚¹çš„å¹³æ»‘è¿‡æ¸¡
        self.TRANSITION_LAYER = [132, 58, 288, 361]  # ä¸‹é¢Œçº¿ä¸­æ®µ
        
        # ç¬¬5å±‚ï¼šä¸»æ§å˜å½¢ç‚¹ï¼ˆå®Œå…¨æ§åˆ¶ï¼‰- æ ¸å¿ƒå˜å½¢åŒºåŸŸ
        self.CHIN_CENTER_SYSTEM = [152, 176, 400, 378]  # 152ä¸»å¯¼ï¼Œå…¶ä»–æ”¯æ’‘
        self.OTHER_CONTOUR = [172, 136, 150, 149, 148, 377, 365, 379, 397, 205, 200, 424, 427]
        
        # ğŸ†• æ–°å¢ï¼šå…³é”®è¿‡æ¸¡æ§åˆ¶ç‚¹ï¼ˆå¡«è¡¥ç©ºç™½åŒºåŸŸï¼‰
        # é¢éƒ¨ç¨³å®šç‚¹ï¼ˆé˜²æ­¢å¤ªé˜³ç©´åŒºåŸŸæ¼‚ç§»ï¼‰
        self.TEMPORAL_STABLE = [162, 21, 54, 389, 251, 284]  # å·¦å³å„3ä¸ª
        
        # ä¸­è„¸è¿‡æ¸¡çº¿ï¼ˆé¢§éª¨åˆ°è„¸é¢Šçš„è¿æ¥ï¼‰
        self.MID_FACE_TRANSITION = [116, 118, 119, 345, 347, 348]  # å·¦å³å„3ä¸ª
        
        # è„¸é¢Šä¸‹ç¼˜è¿‡æ¸¡çº¿ï¼ˆè„¸é¢Šåˆ°ä¸‹é¢Œçš„å…³é”®è¿‡æ¸¡ï¼‰
        self.CHEEK_JAW_TRANSITION = [177, 215, 138, 402, 435, 367]  # å·¦å³å„3ä¸ª
        
        # æ ‡å‡†3Dé¢éƒ¨æ¨¡å‹å…³é”®ç‚¹
        self.face_model_3d = np.array([
            [0.0, 0.0, 0.0],           # é¼»å°– (1)
            [0.0, -330.0, -65.0],      # ä¸‹å·´ (152)  
            [-225.0, 170.0, -135.0],   # å·¦çœ¼å¤–è§’ (33)
            [225.0, 170.0, -135.0],    # å³çœ¼å¤–è§’ (263)
            [-150.0, -150.0, -125.0],  # å·¦å˜´è§’ (61)
            [150.0, -150.0, -125.0],   # å³å˜´è§’ (291)
        ], dtype=np.float32)
        
        # å¯¹åº”çš„MediaPipeç´¢å¼•
        self.pnp_indices = [1, 152, 33, 263, 61, 291]
    
    def setup_camera(self, image_shape):
        """è®¾ç½®ç›¸æœºå‚æ•°"""
        h, w = image_shape[:2]
        focal_length = w
        center = (w/2, h/2)
        
        self.camera_matrix = np.array([
            [focal_length, 0, center[0]],
            [0, focal_length, center[1]],
            [0, 0, 1]
        ], dtype=np.float32)
        
        self.dist_coeffs = np.zeros((4,1), dtype=np.float32)
    
    def detect_landmarks_3d(self, image):
        """æ£€æµ‹é¢éƒ¨å…³é”®ç‚¹å¹¶è®¡ç®—3Då§¿æ€"""
        rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        results = self.face_mesh.process(rgb_image)
        
        if not results.multi_face_landmarks:
            return None, None
        
        landmarks_2d = []
        h, w = image.shape[:2]
        for landmark in results.multi_face_landmarks[0].landmark:
            x = int(landmark.x * w)
            y = int(landmark.y * h)
            landmarks_2d.append([x, y])
        
        landmarks_2d = np.array(landmarks_2d, dtype=np.float32)
        
        if self.camera_matrix is None:
            self.setup_camera(image.shape)
        
        # æå–ç”¨äºPnPçš„å…³é”®ç‚¹
        image_points = []
        for idx in self.pnp_indices:
            if idx < len(landmarks_2d):
                image_points.append(landmarks_2d[idx])
        
        if len(image_points) < 6:
            print("PnPå…³é”®ç‚¹ä¸è¶³")
            return landmarks_2d, None
        
        image_points = np.array(image_points, dtype=np.float32)
        
        # PnPæ±‚è§£3Då§¿æ€
        try:
            success, rotation_vector, translation_vector = cv2.solvePnP(
                self.face_model_3d, image_points, 
                self.camera_matrix, self.dist_coeffs,
                flags=cv2.SOLVEPNP_ITERATIVE
            )
            
            if not success:
                print("PnPæ±‚è§£å¤±è´¥")
                return landmarks_2d, None
            
            return landmarks_2d, (rotation_vector, translation_vector)
            
        except Exception as e:
            print(f"PnPæ±‚è§£å¼‚å¸¸: {e}")
            return landmarks_2d, None
    
    def estimate_face_pose_3d(self, rotation_vector):
        """åŸºäº3Dæ—‹è½¬å‘é‡ä¼°è®¡é¢éƒ¨å§¿æ€"""
        if rotation_vector is None:
            return "frontal"
        
        rotation_matrix, _ = cv2.Rodrigues(rotation_vector)
        sy = np.sqrt(rotation_matrix[0,0] * rotation_matrix[0,0] + rotation_matrix[1,0] * rotation_matrix[1,0])
        
        if sy > 1e-6:
            y = np.arctan2(-rotation_matrix[2,0], sy)
        else:
            y = np.arctan2(-rotation_matrix[2,0], sy)
        
        yaw = np.degrees(y)
        print(f"3Då§¿æ€æ£€æµ‹: Yaw = {yaw:.1f}åº¦")
        
        if abs(yaw) < 15:
            return "frontal"
        elif yaw > 15:
            return "right_profile"
        else:
            return "left_profile"
    
    def get_dynamic_anchors(self, transform_type, pose_3d):
        """æ ¹æ®å˜å½¢ç±»å‹å’Œå§¿æ€åŠ¨æ€é€‰æ‹©é”šç‚¹"""
        base_anchors = self.DYNAMIC_ANCHORS.copy()
        
        print(f"ğŸ”„ åŠ¨æ€é”šç‚¹ç­–ç•¥: {transform_type} + {pose_3d}")
        
        if transform_type == "width":
            selected = [93, 323]
            print(f"   å®½åº¦æ¨¡å¼ï¼šé€‰æ‹©ä¸Šæ–¹é”šç‚¹ {selected}")
            return selected
            
        elif pose_3d == "left_profile":
            selected = [454, 323]
            print(f"   å·¦è½¬å¤´åƒï¼šé€‰æ‹©å³ä¾§å¯è§ç‚¹ {selected}")
            return selected
            
        elif pose_3d == "right_profile":
            selected = [234, 93]
            print(f"   å³è½¬å¤´åƒï¼šé€‰æ‹©å·¦ä¾§å¯è§ç‚¹ {selected}")
            return selected
        else:
            print(f"   æ­£è„¸æ¨¡å¼ï¼šå¯ç”¨å…¨éƒ¨é”šç‚¹ {base_anchors}")
            return base_anchors
    
    def calculate_follow_movement(self, point, chin_center_152, main_transform_vector, follow_ratio, pose_3d, w):
        """è®¡ç®—æ§åˆ¶ç‚¹çš„åè°ƒè·Ÿéšç§»åŠ¨"""
        # è·å–ç›¸å¯¹ä½ç½®
        relative_vector = point - chin_center_152
        distance_from_152 = np.linalg.norm(relative_vector)
        is_left_side = point[0] < chin_center_152[0]
        
        # 3Då§¿æ€è°ƒæ•´
        pose_multiplier = 1.0
        if pose_3d == "left_profile" and is_left_side:
            pose_multiplier = 0.8  # ä¸å¯è§ä¾§é™ä½
        elif pose_3d == "right_profile" and not is_left_side:
            pose_multiplier = 0.8  # ä¸å¯è§ä¾§é™ä½
        
        # è·ç¦»è¡°å‡ï¼ˆç¦»152è¶Šè¿œï¼Œè·Ÿéšè¶Šå¼±ï¼‰
        max_distance = w * 0.2
        distance_ratio = min(distance_from_152 / max_distance, 1.0)
        distance_factor = 1.0 - distance_ratio * 0.3
        
        # è®¡ç®—æœ€ç»ˆè·Ÿéšç§»åŠ¨
        final_follow_ratio = follow_ratio * pose_multiplier * distance_factor
        follow_movement = np.array(main_transform_vector) * final_follow_ratio
        
        return point + follow_movement
    
    def create_optimized_chin_mask(self, image_shape, landmarks_2d, pose_3d):
        """åŸºäºå˜å½¢ç‚¹åˆ›å»ºä¼˜åŒ–çš„ä¸‹å·´mask"""
        h, w = image_shape[:2]
        mask = np.zeros((h, w), dtype=np.uint8)
        
        # åŸºäºæ‰€æœ‰å˜å½¢ç›¸å…³çš„ç‚¹åˆ›å»ºmask
        mask_points = []
        
        # åŒ…å«ä¸»æ§å˜å½¢ç‚¹å’Œè½®å»“ç‚¹
        for idx in self.CHIN_CENTER_SYSTEM + self.OTHER_CONTOUR:
            if idx < len(landmarks_2d):
                mask_points.append(landmarks_2d[idx])
        
        if len(mask_points) >= 3:
            mask_points = np.array(mask_points, dtype=np.int32)
            hull = cv2.convexHull(mask_points)
            
            # è®¡ç®—maskåŒºåŸŸçš„å®é™…å°ºå¯¸
            mask_center = np.mean(hull.reshape(-1, 2), axis=0)
            
            # ä¼˜åŒ–æ‰©å±•ç­–ç•¥
            if pose_3d == "frontal":
                horizontal_expand = 1.4
                vertical_expand = 1.4
            else:  # ä¾§è„¸
                horizontal_expand = 1.2
                vertical_expand = 1.2
            
            # æ‰©å±•mask
            expanded_hull = []
            for point in hull.reshape(-1, 2):
                dx = point[0] - mask_center[0]
                dy = point[1] - mask_center[1]
                
                new_x = mask_center[0] + dx * horizontal_expand
                new_y = mask_center[1] + dy * vertical_expand
                
                expanded_hull.append([new_x, new_y])
            
            expanded_hull = np.array(expanded_hull, dtype=np.int32)
            cv2.fillPoly(mask, [expanded_hull], 255)
            
            # è¾¹ç•Œä¿æŠ¤
            mouth_y = max(landmarks_2d[61][1], landmarks_2d[291][1]) if 61 < len(landmarks_2d) and 291 < len(landmarks_2d) else h//2
            mask_top = np.min(expanded_hull[:, 1])
            
            if mask_top < mouth_y + 8:
                mask[:int(mouth_y + 8), :] = 0
                print("âš ï¸  Maskä¸Šè¾¹ç•Œä¿æŠ¤")
            
            print(f"âœ… ä¼˜åŒ–Mask: å§¿æ€={pose_3d}, æ‰©å±•={horizontal_expand:.1f}x{vertical_expand:.1f}")
        
        return mask
    
    def apply_optimized_chin_transform(self, image, landmarks_2d, pose_info, transform_type="length", intensity=0.3):
        """å¢å¼ºç‰ˆä¸‹å·´å˜å½¢ - å¢åŠ å…³é”®è¿‡æ¸¡æ§åˆ¶ç‚¹å®ç°æ•´ä½“åè°ƒ"""
        if pose_info is None:
            print("3Då§¿æ€ä¿¡æ¯æ— æ•ˆï¼Œä½¿ç”¨2Dæ¨¡å¼")
            return image
        
        rotation_vector, translation_vector = pose_info
        pose_3d = self.estimate_face_pose_3d(rotation_vector)
        
        # 3Då§¿æ€å¼ºåº¦è°ƒæ•´
        if pose_3d != "frontal":
            intensity = intensity * 0.75
            print(f"3Dä¾§è„¸æ£€æµ‹ï¼Œå¼ºåº¦è°ƒæ•´åˆ°: {intensity:.2f}")
        
        # è·å–åŠ¨æ€é”šç‚¹
        active_dynamic_anchors = self.get_dynamic_anchors(transform_type, pose_3d)
        
        # æ„å»ºå¢å¼ºæ§åˆ¶ç‚¹ç³»ç»Ÿ
        src_points = []
        dst_points = []
        h, w = image.shape[:2]
        
        print(f"ğŸ¯ å¢å¼ºç‰ˆå˜å½¢ç³»ç»Ÿ - {transform_type} (å…³é”®è¿‡æ¸¡æ§åˆ¶ç‚¹):")
        
        # è·å–152å·ç‚¹ä½œä¸ºä¸»å¯¼å‚è€ƒç‚¹
        chin_center_152 = landmarks_2d[152] if 152 < len(landmarks_2d) else np.array([w//2, h*0.8])
        
        # è®¡ç®—ä¸»å˜å½¢å‘é‡ï¼ˆ152çš„ç§»åŠ¨å‘é‡ï¼‰
        main_transform_vector = [0, 0]
        if transform_type == "length":
            main_transform_vector = [0, intensity * 18]
        elif transform_type == "width":
            main_transform_vector = [intensity * 4, 0]
        elif transform_type == "sharp":
            main_transform_vector = [0, intensity * 20]
        elif transform_type == "round":
            main_transform_vector = [0, -intensity * 15]
        
        print(f"   152ä¸»å¯¼å˜å½¢å‘é‡: {main_transform_vector}")
        
        # ğŸ”§ ç¬¬1å±‚ï¼šç¡¬é”šç‚¹ï¼ˆç»å¯¹ç¨³å®šï¼Œ0%å˜å½¢ï¼‰
        hard_count = 0
        for idx in self.HARD_ANCHORS:
            if idx < len(landmarks_2d):
                point = landmarks_2d[idx]
                src_points.append(point)
                dst_points.append(point)  # ç»å¯¹ä¸å˜
                hard_count += 1
        print(f"   ç¬¬1å±‚-ç¡¬é”šç‚¹: {hard_count}ä¸ª (0%å˜å½¢)")
        
        # ğŸ†• æ–°å¢å±‚ï¼šé¢éƒ¨ç¨³å®šç‚¹ï¼ˆ5-8%å¾®è·Ÿéšï¼‰
        temporal_count = 0
        temporal_follow_ratio = 0.06  # 6%è·Ÿéš
        
        for idx in self.TEMPORAL_STABLE:
            if idx < len(landmarks_2d):
                point = landmarks_2d[idx]
                src_points.append(point)
                
                new_point = self.calculate_follow_movement(
                    point, chin_center_152, main_transform_vector, 
                    temporal_follow_ratio, pose_3d, w
                )
                dst_points.append(new_point)
                temporal_count += 1
        print(f"   æ–°å¢-é¢éƒ¨ç¨³å®š: {temporal_count}ä¸ª ({temporal_follow_ratio*100:.0f}%åè°ƒè·Ÿéš)")
        
        # ğŸ”§ ç¬¬2å±‚ï¼šå¼±é”šç‚¹ï¼ˆå¾®è·Ÿéšç¨³å®šï¼Œ8%å˜å½¢ï¼‰
        weak_count = 0
        weak_follow_ratio = 0.08
        
        for idx in self.WEAK_ANCHORS:
            if idx < len(landmarks_2d):
                point = landmarks_2d[idx]
                src_points.append(point)
                
                new_point = self.calculate_follow_movement(
                    point, chin_center_152, main_transform_vector, 
                    weak_follow_ratio, pose_3d, w
                )
                dst_points.append(new_point)
                weak_count += 1
        print(f"   ç¬¬2å±‚-å¼±é”šç‚¹: {weak_count}ä¸ª ({weak_follow_ratio*100:.0f}%åè°ƒè·Ÿéš)")
        
        # ğŸ†• æ–°å¢å±‚ï¼šä¸­è„¸è¿‡æ¸¡çº¿ï¼ˆ15-20%è·Ÿéšï¼‰
        mid_face_count = 0
        mid_face_follow_ratio = 0.18  # 18%è·Ÿéš
        
        for idx in self.MID_FACE_TRANSITION:
            if idx < len(landmarks_2d):
                point = landmarks_2d[idx]
                src_points.append(point)
                
                new_point = self.calculate_follow_movement(
                    point, chin_center_152, main_transform_vector, 
                    mid_face_follow_ratio, pose_3d, w
                )
                dst_points.append(new_point)
                mid_face_count += 1
        print(f"   æ–°å¢-ä¸­è„¸è¿‡æ¸¡: {mid_face_count}ä¸ª ({mid_face_follow_ratio*100:.0f}%åè°ƒè·Ÿéš)")
        
        # ğŸ†• æ–°å¢å±‚ï¼šè„¸é¢Šä¸‹ç¼˜è¿‡æ¸¡çº¿ï¼ˆ25-40%è·Ÿéšï¼‰
        cheek_jaw_count = 0
        cheek_jaw_follow_ratio = 0.38  # 38%è·Ÿéš
        
        for idx in self.CHEEK_JAW_TRANSITION:
            if idx < len(landmarks_2d):
                point = landmarks_2d[idx]
                src_points.append(point)
                
                new_point = self.calculate_follow_movement(
                    point, chin_center_152, main_transform_vector, 
                    cheek_jaw_follow_ratio, pose_3d, w
                )
                dst_points.append(new_point)
                cheek_jaw_count += 1
        print(f"   æ–°å¢-è„¸é¢Šä¸‹ç¼˜: {cheek_jaw_count}ä¸ª ({cheek_jaw_follow_ratio*100:.0f}%åè°ƒè·Ÿéš)")
        
        # ğŸ”§ ç¬¬3å±‚ï¼šåŠ¨æ€é”šç‚¹ï¼ˆä»»åŠ¡è‡ªé€‚åº”ï¼‰
        dynamic_count = 0
        for idx in active_dynamic_anchors:
            if idx < len(landmarks_2d):
                point = landmarks_2d[idx]
                src_points.append(point)
                
                # æ ¹æ®å˜å½¢ç±»å‹å†³å®šæ˜¯å¦è·Ÿéš
                if transform_type == "width" and abs(intensity) > 0.2:
                    is_left_side = point[0] < chin_center_152[0]
                    micro_follow = 1.5 if (intensity > 0) == is_left_side else -1.5
                    new_point = [point[0] + micro_follow, point[1]]
                    dst_points.append(new_point)
                else:
                    dst_points.append(point)
                dynamic_count += 1
        print(f"   ç¬¬3å±‚-åŠ¨æ€é”šç‚¹: {dynamic_count}ä¸ª (è‡ªé€‚åº”)")
        
        # ğŸ”§ ç¬¬4å±‚ï¼šè¿‡æ¸¡å±‚ï¼ˆæ¸å˜è·Ÿéšï¼Œ55%å˜å½¢ï¼‰
        transition_count = 0
        transition_ratio = 0.55
        
        for idx in self.TRANSITION_LAYER:
            if idx < len(landmarks_2d):
                point = landmarks_2d[idx]
                src_points.append(point)
                
                new_point = self.calculate_follow_movement(
                    point, chin_center_152, main_transform_vector, 
                    transition_ratio, pose_3d, w
                )
                dst_points.append(new_point)
                transition_count += 1
        print(f"   ç¬¬4å±‚-è¿‡æ¸¡å±‚: {transition_count}ä¸ª ({transition_ratio*100:.0f}%åè°ƒè·Ÿéš)")
        
        # ğŸ”§ ç¬¬5å±‚ï¼šä¸»æ§å˜å½¢ç‚¹ï¼ˆå®Œå…¨æ§åˆ¶ï¼Œ100%å˜å½¢ï¼‰
        main_count = 0
        
        # 152ä¸»å¯¼å˜å½¢
        if 152 < len(landmarks_2d):
            point = landmarks_2d[152]
            src_points.append(point)
            new_point = point + np.array(main_transform_vector)
            dst_points.append(new_point)
            main_count += 1
            print(f"     152ä¸»å¯¼ç§»åŠ¨: {main_transform_vector}")
        
        # ä¸­è½´ç³»ç»Ÿåè°ƒè·Ÿéš
        for idx in [176, 400, 378]:
            if idx < len(landmarks_2d):
                point = landmarks_2d[idx]
                src_points.append(point)
                
                # åè°ƒè·Ÿéš152çš„ä¸»å¯¼å˜å½¢
                relative_vector = point - chin_center_152
                distance_from_152 = np.linalg.norm(relative_vector)
                follow_ratio = 0.7 * max(0.3, 1 - distance_from_152 / (w * 0.1))
                
                coordinated_movement = np.array(main_transform_vector) * follow_ratio
                new_point = point + coordinated_movement
                dst_points.append(new_point)
                main_count += 1
        
        # å…¶ä»–è½®å»“ç‚¹åè°ƒè·Ÿéš
        for idx in self.OTHER_CONTOUR:
            if idx < len(landmarks_2d):
                point = landmarks_2d[idx]
                src_points.append(point)
                
                new_point = self.calculate_follow_movement(
                    point, chin_center_152, main_transform_vector, 
                    0.5, pose_3d, w  # 50%åè°ƒè·Ÿéš
                )
                dst_points.append(new_point)
                main_count += 1
        
        print(f"   ç¬¬5å±‚-ä¸»æ§å˜å½¢: {main_count}ä¸ª (åè°ƒä¸»å¯¼æ¨¡å¼)")
        
        # æ·»åŠ å…¶ä»–ç¨³å®šåŒºåŸŸï¼ˆçœ¼éƒ¨ã€é¼»å­ã€å˜´éƒ¨ï¼‰
        other_stable = [33, 133, 362, 263, 1, 2, 5, 61, 291, 9, 151]
        other_count = 0
        for idx in other_stable:
            if idx < len(landmarks_2d):
                src_points.append(landmarks_2d[idx])
                dst_points.append(landmarks_2d[idx])
                other_count += 1
        print(f"   å…¶ä»–ç¨³å®šåŒºåŸŸ: {other_count}ä¸ª")
        
        # æ·»åŠ è¾¹ç•Œé”šç‚¹
        boundary_margin = 25
        boundary_points = [
            [boundary_margin, boundary_margin], 
            [w//2, boundary_margin], 
            [w-boundary_margin, boundary_margin],
            [boundary_margin, h//2], 
            [w-boundary_margin, h//2],
            [boundary_margin, h-boundary_margin], 
            [w//2, h-boundary_margin], 
            [w-boundary_margin, h-boundary_margin]
        ]
        
        for bp in boundary_points:
            src_points.append(bp)
            dst_points.append(bp)
        
        total_points = len(src_points)
        print(f"   è¾¹ç•Œä¿æŠ¤: 8ä¸ª")
        print(f"   æ€»è®¡æ§åˆ¶ç‚¹: {total_points}ä¸ª (å¢å¼ºå‰çº¦20ä¸ª)")
        
        if total_points < 25:
            print("âŒ æ§åˆ¶ç‚¹ä¸è¶³")
            return image
        
        # æ‰§è¡ŒTPSå˜æ¢
        try:
            src_points = np.array(src_points, dtype=np.float32)
            dst_points = np.array(dst_points, dtype=np.float32)
            
            tps = cv2.createThinPlateSplineShapeTransformer()
            matches = [cv2.DMatch(i, i, 0) for i in range(len(src_points))]
            
            tps.estimateTransformation(
                dst_points.reshape(1, -1, 2), 
                src_points.reshape(1, -1, 2), 
                matches
            )
            
            # TPSå˜å½¢
            transformed = tps.warpImage(image)
            
            if transformed is None:
                print("âŒ TPSå˜æ¢å¤±è´¥")
                return image
            
            # åˆ›å»ºä¼˜åŒ–mask
            chin_mask = self.create_optimized_chin_mask(image.shape, landmarks_2d, pose_3d)
            
            # èåˆ
            mask_3d = cv2.merge([chin_mask, chin_mask, chin_mask]).astype(np.float32) / 255.0
            kernel_size = max(17, min(w, h) // 45)
            if kernel_size % 2 == 0:
                kernel_size += 1
            mask_blurred = cv2.GaussianBlur(mask_3d, (kernel_size, kernel_size), kernel_size//3)
            
            result = image.astype(np.float32) * (1 - mask_blurred) + transformed.astype(np.float32) * mask_blurred
            result = np.clip(result, 0, 255).astype(np.uint8)
            
            print("âœ… ä¼˜åŒ–ç‰ˆåè°ƒå˜å½¢æˆåŠŸ")
            return result
            
        except Exception as e:
            print(f"âŒ ä¼˜åŒ–ç‰ˆå˜å½¢å¼‚å¸¸: {e}")
            return image
    
    def debug_optimized_detection(self, image, save_dir=None):
        """è°ƒè¯•å¢å¼ºç‰ˆçš„å…³é”®ç‚¹æ£€æµ‹"""
        landmarks_2d, pose_info = self.detect_landmarks_3d(image)
        if landmarks_2d is None:
            print("æœªæ£€æµ‹åˆ°é¢éƒ¨å…³é”®ç‚¹")
            return image
        
        debug_img = image.copy()
        
        # ğŸ¯ æŒ‰å¢å¼ºæ¶æ„ç»˜åˆ¶å…³é”®ç‚¹
        layer_configs = [
            (self.HARD_ANCHORS, (255, 255, 0), "HARD", 10),                    # ç¡¬é”šç‚¹-é»„è‰²-å¤§åœ†
            (self.TEMPORAL_STABLE, (128, 255, 255), "TEMP", 7),                # é¢éƒ¨ç¨³å®š-æµ…é’è‰²
            (self.WEAK_ANCHORS, (0, 255, 255), "WEAK", 8),                     # å¼±é”šç‚¹-é’è‰²-ä¸­åœ†  
            (self.MID_FACE_TRANSITION, (255, 128, 0), "MID", 6),               # ä¸­è„¸è¿‡æ¸¡-æ©™è‰²
            (self.CHEEK_JAW_TRANSITION, (255, 0, 128), "CHEEK", 6),            # è„¸é¢Šä¸‹ç¼˜-ç²‰è‰²
            (self.DYNAMIC_ANCHORS, (255, 0, 255), "DYN", 7),                   # åŠ¨æ€é”šç‚¹-ç´«è‰²-ä¸­åœ†
            (self.TRANSITION_LAYER, (0, 255, 0), "TRANS", 6),                  # è¿‡æ¸¡å±‚-ç»¿è‰²-å°åœ†
            (self.CHIN_CENTER_SYSTEM, (0, 0, 255), "CENTER", 8),               # ä¸­è½´ç³»ç»Ÿ-çº¢è‰²-ä¸­åœ†
            (self.OTHER_CONTOUR, (255, 0, 0), "CONT", 5),                      # å…¶ä»–è½®å»“-è“è‰²-å°åœ†
        ]
        
        for indices, color, prefix, radius in layer_configs:
            for i, idx in enumerate(indices):
                if idx < len(landmarks_2d):
                    point = landmarks_2d[idx].astype(int)
                    
                    # 152ç”¨ç‰¹å¤§åœ†çªå‡ºæ˜¾ç¤º
                    if idx == 152:
                        radius = 12
                    
                    cv2.circle(debug_img, tuple(point), radius, color, -1)
                    cv2.circle(debug_img, tuple(point), radius + 2, color, 2)
                    cv2.putText(debug_img, f"{prefix}{idx}", tuple(point + 12), 
                            cv2.FONT_HERSHEY_SIMPLEX, 0.4, color, 1)
        
        # æ˜¾ç¤º3Då§¿æ€ä¿¡æ¯
        if pose_info is not None:
            rotation_vector, translation_vector = pose_info
            pose_3d = self.estimate_face_pose_3d(rotation_vector)
            cv2.putText(debug_img, f"3D Pose: {pose_3d}", (10, 30), 
                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
        
        # æ˜¾ç¤ºå¢å¼ºæ¶æ„ç»Ÿè®¡
        total_new_points = len(self.TEMPORAL_STABLE) + len(self.MID_FACE_TRANSITION) + len(self.CHEEK_JAW_TRANSITION)
        stats_text = f"Enhanced: +{total_new_points} transition points (TEMP:{len(self.TEMPORAL_STABLE)} MID:{len(self.MID_FACE_TRANSITION)} CHEEK:{len(self.CHEEK_JAW_TRANSITION)})"
        cv2.putText(debug_img, stats_text, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)
        
        # æ˜¾ç¤ºä¼˜åŒ–mask
        if pose_info is not None:
            pose_3d = self.estimate_face_pose_3d(pose_info[0])
            chin_mask = self.create_optimized_chin_mask(image.shape, landmarks_2d, pose_3d)
            mask_overlay = debug_img.copy()
            mask_overlay[chin_mask > 0] = [128, 0, 128]  # æ·±ç´«è‰²mask
            debug_img = cv2.addWeighted(debug_img, 0.75, mask_overlay, 0.25, 0)
        
        # æ›´æ–°å›¾ä¾‹è¯´æ˜ï¼ˆå¢å¼ºæ¶æ„ï¼‰
        legend_y = 90
        cv2.putText(debug_img, "YELLOW: Hard Anchors (0%)", (10, legend_y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1)
        cv2.putText(debug_img, "LIGHT_CYAN: Temporal Stable (6%)", (10, legend_y + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (128, 255, 255), 1)
        cv2.putText(debug_img, "CYAN: Weak Anchors (8%)", (10, legend_y + 40), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 1)
        cv2.putText(debug_img, "ORANGE: Mid Face Transition (18%)", (10, legend_y + 60), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 128, 0), 1)
        cv2.putText(debug_img, "PINK: Cheek-Jaw Transition (28%)", (10, legend_y + 80), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 128), 1)
        cv2.putText(debug_img, "MAGENTA: Dynamic Anchors", (10, legend_y + 100), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 255), 1)
        cv2.putText(debug_img, "GREEN: Transition Layer (40%)", (10, legend_y + 120), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)
        cv2.putText(debug_img, "RED: Main Control (100%)", (10, legend_y + 140), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)
        cv2.putText(debug_img, "BLUE: Other Contour (50%)", (10, legend_y + 160), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)
        
        # ç»˜åˆ¶åè°ƒè·Ÿéšè¿æ¥çº¿
        if 152 < len(landmarks_2d):
            chin_152 = landmarks_2d[152].astype(int)
            
            # è¿æ¥æ–°å¢çš„è¿‡æ¸¡ç‚¹åˆ°152ï¼Œæ˜¾ç¤ºåè°ƒå…³ç³»
            for transition_points, color in [(self.TEMPORAL_STABLE, (128, 255, 255)), 
                                           (self.MID_FACE_TRANSITION, (255, 128, 0)),
                                           (self.CHEEK_JAW_TRANSITION, (255, 0, 128))]:
                for idx in transition_points:
                    if idx < len(landmarks_2d):
                        point = landmarks_2d[idx].astype(int)
                        # ç»˜åˆ¶è™šçº¿è¡¨ç¤ºåè°ƒå…³ç³»
                        cv2.line(debug_img, tuple(point), tuple(chin_152), color, 1, cv2.LINE_AA)
        
        if save_dir:
            os.makedirs(save_dir, exist_ok=True)
            cv2.imwrite(os.path.join(save_dir, "debug_chin_optimized.png"), debug_img)
            print(f"ä¼˜åŒ–ç‰ˆè°ƒè¯•æ–‡ä»¶ä¿å­˜åˆ°: {save_dir}")
        
        return debug_img
    
    def process_image(self, image, transform_type="length", intensity=0.3, save_debug=False, output_path=None):
        """å¤„ç†å›¾åƒä¸»å‡½æ•°"""
        landmarks_2d, pose_info = self.detect_landmarks_3d(image)
        if landmarks_2d is None:
            print("æœªæ£€æµ‹åˆ°é¢éƒ¨å…³é”®ç‚¹")
            return image
        
        print(f"âœ… æ£€æµ‹åˆ° {len(landmarks_2d)} ä¸ª2Då…³é”®ç‚¹")
        
        if save_debug and output_path:
            debug_dir = os.path.splitext(output_path)[0] + "_debug"
            self.debug_optimized_detection(image, debug_dir)
        
        return self.apply_optimized_chin_transform(image, landmarks_2d, pose_info, transform_type, intensity)

def main():
    parser = argparse.ArgumentParser(description='å¢å¼ºç‰ˆä¸‹å·´è°ƒæ•´å·¥å…· - å…³é”®è¿‡æ¸¡æ§åˆ¶ç‚¹è§£å†³æ•´ä½“åè°ƒæ€§')
    parser.add_argument('--input', '-i', required=True, help='è¾“å…¥å›¾ç‰‡è·¯å¾„')
    parser.add_argument('--output', '-o', required=True, help='è¾“å‡ºå›¾ç‰‡è·¯å¾„')
    parser.add_argument('--type', '-t', choices=['length', 'width', 'sharp', 'round'], 
                       default='length', help='è°ƒæ•´ç±»å‹ï¼šlength=é•¿åº¦, width=å®½åº¦, sharp=å°–é”, round=åœ†æ¶¦')
    parser.add_argument('--intensity', type=float, default=0.3, help='è°ƒæ•´å¼ºåº¦ (0.1-0.5)')
    parser.add_argument('--debug', action='store_true', help='å¢å¼ºæ¶æ„è°ƒè¯•æ¨¡å¼')
    parser.add_argument('--save-debug', action='store_true', help='ä¿å­˜è°ƒè¯•æ–‡ä»¶')
    parser.add_argument('--comparison', action='store_true', help='ç”Ÿæˆå¯¹æ¯”å›¾')
    
    args = parser.parse_args()
    
    # åŠ è½½å›¾ç‰‡
    image = cv2.imread(args.input)
    if image is None:
        print(f"æ— æ³•åŠ è½½å›¾ç‰‡: {args.input}")
        return
    
    print(f"å›¾ç‰‡å°ºå¯¸: {image.shape}")
    
    # åˆ›å»ºä¼˜åŒ–å¤„ç†å™¨
    processor = OptimizedChin3DAdjustment()
    
    # è°ƒè¯•æ¨¡å¼
    if args.debug:
        debug_result = processor.debug_optimized_detection(image)
        cv2.imwrite(args.output, debug_result)
        print(f"å¢å¼ºæ¶æ„è°ƒè¯•å›¾ä¿å­˜åˆ°: {args.output}")
        return
    
    # å¼ºåº¦å®‰å…¨æ£€æŸ¥
    if args.intensity > 0.5:
        print(f"è­¦å‘Šï¼šå¼ºåº¦ {args.intensity} å¯èƒ½é€ æˆæ‰­æ›²ï¼Œè‡ªåŠ¨é™åˆ¶åˆ°0.5")
        args.intensity = 0.5
    elif args.intensity < -0.5:
        print(f"è­¦å‘Šï¼šå¼ºåº¦ {args.intensity} å¯èƒ½é€ æˆæ‰­æ›²ï¼Œè‡ªåŠ¨é™åˆ¶åˆ°-0.5")
        args.intensity = -0.5
    
    # å¤„ç†å›¾ç‰‡
    print(f"å¼€å§‹å¢å¼ºç‰ˆå¤„ç† - ç±»å‹: {args.type}, å¼ºåº¦: {args.intensity}")
    result = processor.process_image(image, args.type, args.intensity, args.save_debug, args.output)
    
    # ä¿å­˜ç»“æœ
    cv2.imwrite(args.output, result)
    print(f"å¤„ç†å®Œæˆï¼Œä¿å­˜åˆ°: {args.output}")
    
    # æ£€æŸ¥å˜åŒ–
    diff = cv2.absdiff(image, result)
    total_diff = np.sum(diff)
    changed_pixels = np.sum(diff > 0)
    max_diff = np.max(diff)
    print(f"ğŸ“Š åƒç´ å˜åŒ–ç»Ÿè®¡:")
    print(f"   æ€»å·®å¼‚: {total_diff}")
    print(f"   æœ€å¤§å·®å¼‚: {max_diff}")
    print(f"   å˜åŒ–åƒç´ æ•°: {changed_pixels}")
    
    if total_diff < 1000:
        print("âš ï¸  å˜åŒ–å¾ˆå°ï¼Œå¯èƒ½éœ€è¦ï¼š")
        print("   1. æé«˜ --intensity å‚æ•°")
        print("   2. æ£€æŸ¥å…³é”®ç‚¹æ˜¯å¦æ­£ç¡®æ£€æµ‹")
        print("   3. å°è¯•ä¸åŒçš„å˜å½¢ç±»å‹")
    
    # ç”Ÿæˆå¯¹æ¯”å›¾
    if args.comparison:
        comparison_path = args.output.replace('.', '_enhanced_comparison.')
        comparison = np.hstack([image, result])
        cv2.line(comparison, (image.shape[1], 0), (image.shape[1], image.shape[0]), (0, 255, 0), 3)
        
        # æ·»åŠ æ ‡ç­¾
        cv2.putText(comparison, 'Original', (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 3)
        cv2.putText(comparison, f'Enhanced-{args.type.upper()} {args.intensity}', (image.shape[1] + 20, 50), 
                   cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 3)
        
        cv2.imwrite(comparison_path, comparison)
        print(f"å¢å¼ºç‰ˆå¯¹æ¯”å›¾ä¿å­˜åˆ°: {comparison_path}")
    
    print("\nâœ… å¢å¼ºç‰ˆç‰¹æ€§:")
    print("ğŸ†• æ–°å¢å±‚çº§:")
    print("   â€¢ é¢éƒ¨ç¨³å®šç‚¹: 6%åè°ƒè·Ÿéš - é˜²æ­¢å¤ªé˜³ç©´æ¼‚ç§»")
    print("   â€¢ ä¸­è„¸è¿‡æ¸¡çº¿: 18%åè°ƒè·Ÿéš - é¢§éª¨åˆ°è„¸é¢Šè¿æ¥")
    print("   â€¢ è„¸é¢Šä¸‹ç¼˜è¿‡æ¸¡: 28%åè°ƒè·Ÿéš - è„¸é¢Šåˆ°ä¸‹é¢Œå…³é”®è¿‡æ¸¡")
    print("\nğŸ”§ åŸæœ‰5å±‚æ¶æ„ä¿æŒ:")
    print("   â€¢ ç¡¬é”šç‚¹: 0%å˜å½¢ (ç»å¯¹ç¨³å®š)")
    print("   â€¢ å¼±é”šç‚¹: 8%åè°ƒè·Ÿéš (å¾®è·Ÿéš)")
    print("   â€¢ åŠ¨æ€é”šç‚¹: è‡ªé€‚åº”é€‰æ‹©")
    print("   â€¢ è¿‡æ¸¡å±‚: 40%åè°ƒè·Ÿéš")
    print("   â€¢ ä¸»æ§å˜å½¢: 100%ç²¾ç¡®æ§åˆ¶")
    print("\nğŸ¯ è§£å†³çš„å…³é”®é—®é¢˜:")
    print("   âœ… æ¶ˆé™¤ç©ºç™½åŒºåŸŸï¼šå…³é”®è¿‡æ¸¡çº¿å…¨è¦†ç›–")
    print("   âœ… æ•´ä½“åè°ƒæ€§ï¼šå…¨è„¸è½»å¾®è·Ÿéšä¸‹å·´å˜å½¢")
    print("   âœ… è¾¹ç¼˜è¿ç»­æ€§ï¼šå¹³æ»‘çš„å˜å½¢æ¢¯åº¦åˆ†å¸ƒ")
    print("   âœ… ä¿æŒå˜å½¢æ•ˆæœï¼šä¸‹å·´å˜å½¢å¼ºåº¦å®Œå…¨ä¿ç•™")
    print("\nğŸ”¢ æ§åˆ¶ç‚¹ç»Ÿè®¡:")
    print("   â€¢ åŸç‰ˆ: ~20ä¸ªæ§åˆ¶ç‚¹")
    print("   â€¢ å¢å¼ºç‰ˆ: ~32ä¸ªæ§åˆ¶ç‚¹ (æ–°å¢12ä¸ªå…³é”®è¿‡æ¸¡ç‚¹)")
    print("   â€¢ æ‰€æœ‰æ–°å¢ç‚¹éƒ½åŸºäº152ä¸»å¯¼å˜å½¢è¿›è¡Œåè°ƒè®¡ç®—")
    print("\nğŸ“– ä½¿ç”¨ç¤ºä¾‹:")
    print("   â€¢ åè°ƒä¸‹å·´æ‹‰é•¿: python chin_adjust_enhanced.py -i input.jpg -o output.png -t length --intensity 0.3 --comparison")
    print("   â€¢ åè°ƒä¸‹å·´æ”¶çª„: python chin_adjust_enhanced.py -i input.jpg -o output.png -t width --intensity -0.2 --comparison")
    print("   â€¢ æŸ¥çœ‹å¢å¼ºæ¶æ„: python chin_adjust_enhanced.py -i input.jpg -o debug.png --debug")

if __name__ == "__main__":
    main()