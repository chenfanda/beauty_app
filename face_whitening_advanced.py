#!/usr/bin/env python3
"""
è¿›é˜¶ç‰ˆä¸“ä¸šç¾ç™½ç®—æ³• - åˆ†å±‚å¤„ç†+è‡ªé€‚åº”å¢å¼º
åŸºäºå·¥ä¸šç•Œæ ‡å‡†çš„åˆ†å±‚ç¾ç™½æ–¹æ¡ˆï¼Œæ”¯æŒå§¿æ€è‡ªé€‚åº”å’ŒåŒºåŸŸé€‰æ‹©
"""

import cv2
import numpy as np
import mediapipe as mp
import argparse

class AdvancedFaceWhitening:
    def __init__(self, face_mesh=None):
        self.mp_face_mesh = mp.solutions.face_mesh
        if not face_mesh:
            self.face_mesh = self.mp_face_mesh.FaceMesh(
                static_image_mode=False,
                max_num_faces=1,
                refine_landmarks=True,
                min_detection_confidence=0.7,
                min_tracking_confidence=0.5
            )
        else:
            self.face_mesh = face_mesh
        
        # æ ¸å¿ƒç¾ç™½åŒºåŸŸï¼ˆé¢§éª¨ä¸è„¸é¢Šä¸­éƒ¨ - å¼ºåº¦100%ï¼‰
        self.CORE_WHITENING_REGION = [117, 118, 119, 121, 122, 123, 147, 213, 192, 352, 351, 350, 346]
        
        # å®Œæ•´é¢éƒ¨è½®å»“ï¼ˆå¤–å›´åŒºåŸŸ - å¼ºåº¦60-80%ï¼‰
        self.FULL_FACE_CONTOUR = [10, 338, 297, 332, 284, 251, 389, 356, 454, 
                                  323, 361, 288, 397, 365, 379, 378, 400, 377, 
                                  152, 148, 176, 149, 150, 136, 172, 58, 132, 
                                  93, 234, 127, 162, 21, 54, 103, 67, 109]
        
        # è‚¤è‰²ä¿æŠ¤åŒºåŸŸï¼ˆé¿å…è¿‡åº¦å¤„ç†ï¼‰
        self.SKIN_PROTECTION_ANCHORS = [
            # çœ¼éƒ¨åŒºåŸŸ
            33, 7, 163, 144, 145, 153, 154, 155, 133, 173, 157, 158, 159, 160, 161, 246,
            # é¼»éƒ¨åŒºåŸŸ  
            1, 2, 5, 4, 6, 19, 20, 94, 125, 141, 235, 31, 228, 229, 230, 231, 232, 233, 244, 245, 122, 6, 202, 214, 234,
            # å˜´éƒ¨åŒºåŸŸ
            164, 0, 17, 18, 200, 199, 175, 178, 346, 347, 348, 349, 350, 451, 452, 453, 464, 435, 410, 454, 323, 366, 401, 361, 340, 346, 347, 348, 349, 350, 451, 452, 453, 464, 435, 410, 454, 323, 366, 401, 361, 340
        ]
    
    def detect_landmarks(self, image):
        """æ£€æµ‹é¢éƒ¨å…³é”®ç‚¹"""
        rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        results = self.face_mesh.process(rgb_image)
        
        if not results.multi_face_landmarks:
            return None
        
        landmarks = []
        h, w = image.shape[:2]
        for landmark in results.multi_face_landmarks[0].landmark:
            x = int(landmark.x * w)
            y = int(landmark.y * h)
            landmarks.append([x, y])
        
        return np.array(landmarks)
    
    def estimate_face_pose(self, landmarks):
        """
        æ”¹è¿›çš„å§¿æ€ä¼°è®¡ - å‚è€ƒç˜¦è„¸è„šæœ¬é€»è¾‘
        ä½¿ç”¨å¤šä¸ªç‰¹å¾ç‚¹ç»„åˆåˆ¤æ–­ï¼Œæé«˜å‡†ç¡®æ€§
        """
        # å…³é”®ç‰¹å¾ç‚¹
        left_eye_x = landmarks[33][0]
        right_eye_x = landmarks[263][0] 
        nose_tip_x = landmarks[1][0]
        nose_bridge_x = landmarks[6][0]  # é¼»æ¢ä¸­ç‚¹
        
        # é¢éƒ¨ä¸­è½´çº¿ä¼°è®¡
        eye_center_x = (left_eye_x + right_eye_x) / 2
        nose_center_x = (nose_tip_x + nose_bridge_x) / 2
        
        # è®¡ç®—åç§»æ¯”ä¾‹
        nose_offset = nose_center_x - eye_center_x
        eye_distance = abs(right_eye_x - left_eye_x)
        
        if eye_distance > 0:
            yaw_ratio = nose_offset / eye_distance
            
            # å‚è€ƒç˜¦è„¸è„šæœ¬çš„é˜ˆå€¼è®¾ç½®ï¼Œæ›´ä¸¥æ ¼çš„åˆ¤æ–­
            if abs(yaw_ratio) < 0.05:  # æ›´ä¸¥æ ¼çš„æ­£è„¸åˆ¤æ–­
                return "frontal"
            elif yaw_ratio > 0.05:
                return "right_profile" 
            else:
                return "left_profile"
        return "frontal"
    
    def analyze_skin_brightness(self, image, landmarks):
        """åˆ†æé¢éƒ¨åŒºåŸŸäº®åº¦ç”¨äºè‡ªé€‚åº”è°ƒæ•´"""
        # æå–æ ¸å¿ƒåŒºåŸŸçš„äº®åº¦ä¿¡æ¯
        mask = np.zeros(image.shape[:2], dtype=np.uint8)
        core_points = np.array([landmarks[i] for i in self.CORE_WHITENING_REGION if i < len(landmarks)], np.int32)
        
        if len(core_points) > 0:
            cv2.fillConvexPoly(mask, core_points, 255)
            
            # è®¡ç®—LABè‰²å½©ç©ºé—´çš„Lé€šé“å¹³å‡å€¼
            lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
            l_channel = lab_image[:, :, 0]
            mean_brightness = np.mean(l_channel[mask > 0])
            
            return mean_brightness
        return 128  # é»˜è®¤ä¸­ç­‰äº®åº¦
    
    def create_layered_masks(self, image, landmarks):
        """åˆ›å»ºåˆ†å±‚é®ç½©ï¼šæ ¸å¿ƒåŒºåŸŸ+å¤–å›´åŒºåŸŸ - æœ€å°åŒ–ä¿®å¤ç‰ˆ"""
        h, w = image.shape[:2]
        
        # æ ¸å¿ƒé®ç½©ï¼ˆå¼ºåº¦100%ï¼‰
        core_mask = np.zeros((h, w), dtype=np.uint8)
        core_points = np.array([landmarks[i] for i in self.CORE_WHITENING_REGION if i < len(landmarks)], np.int32)
        if len(core_points) > 0:
            # ä¿®å¤ï¼šä½¿ç”¨å‡¸åŒ…ç¡®ä¿è¿ç»­æ€§
            hull = cv2.convexHull(core_points)
            cv2.fillPoly(core_mask, [hull], 255)
        
        # å®Œæ•´é¢éƒ¨é®ç½©
        full_mask = np.zeros((h, w), dtype=np.uint8)
        full_points = np.array([landmarks[i] for i in self.FULL_FACE_CONTOUR if i < len(landmarks)], np.int32)
        if len(full_points) > 0:
            # ä¿®å¤ï¼šä½¿ç”¨å‡¸åŒ…ç¡®ä¿è¿ç»­æ€§
            hull = cv2.convexHull(full_points)
            cv2.fillPoly(full_mask, [hull], 255)
        
        # å¤–å›´é®ç½© = å®Œæ•´é¢éƒ¨ - æ ¸å¿ƒåŒºåŸŸ
        outer_mask = cv2.subtract(full_mask, core_mask)
        
        # é«˜æ–¯æ¨¡ç³Šè½¯åŒ–è¾¹ç•Œ
        core_mask = cv2.GaussianBlur(core_mask, (15, 15), 0)
        outer_mask = cv2.GaussianBlur(outer_mask, (25, 25), 0)
        
        return core_mask, outer_mask
    
    def adaptive_whitening_pipeline(self, face_region, brightness_level, intensity=0.5, region_type="core"):
        """
        è‡ªé€‚åº”ç¾ç™½ç®¡é“å¤„ç† - æ”¹è¿›LABå¤„ç†ï¼Œé¿å…çº¿æ€§ä¸è‡ªç„¶
        """
        if face_region.sum() == 0:
            return face_region
        
        # è½¬æ¢åˆ°LABè‰²å½©ç©ºé—´ï¼ˆæ›´å¥½çš„è‚¤è‰²ä¿æŠ¤ï¼‰
        lab_image = cv2.cvtColor(face_region, cv2.COLOR_BGR2LAB)
        l, a, b = cv2.split(lab_image)
        
        # æ ¹æ®åŒºåŸŸç±»å‹è°ƒæ•´åŸºç¡€å¼ºåº¦
        if region_type == "core":
            base_intensity = intensity
        else:  # outer
            base_intensity = intensity * 0.7  # å¤–å›´åŒºåŸŸå¼ºåº¦é™ä½
        
        # åŸºäºäº®åº¦è‡ªé€‚åº”è°ƒæ•´ï¼ˆæš—éƒ¨å¢å¼ºæ›´å¤šï¼Œäº®éƒ¨é€‚åº¦ï¼‰
        if brightness_level < 100:  # è¾ƒæš—
            brightness_multiplier = 1.3
        elif brightness_level > 160:  # è¾ƒäº®
            brightness_multiplier = 0.8
        else:  # ä¸­ç­‰
            brightness_multiplier = 1.0
        
        final_intensity = base_intensity * brightness_multiplier
        final_intensity = np.clip(final_intensity, 0.1, 0.9)  # å®‰å…¨èŒƒå›´
        
        # ä¿®å¤ï¼šä½¿ç”¨Gammaæ ¡æ­£æ›¿ä»£çº¿æ€§å¤„ç†ï¼Œé¿å…ä¸è‡ªç„¶
        gamma = 1.0 - final_intensity * 0.3
        gamma = max(0.5, gamma)
        
        l_normalized = l.astype(np.float32) / 255.0
        l_enhanced = np.power(l_normalized, gamma)
        l_enhanced = (l_enhanced * 255).astype(np.uint8)
        
        # ä¿®å¤ï¼šæ›´æ¸©å’Œçš„aé€šé“è°ƒæ•´
        a_adjusted = a.astype(np.float32)
        neutral_value = 128.0
        adjustment_factor = final_intensity * 0.02  # é™ä½è°ƒæ•´å¼ºåº¦
        a_adjusted = a_adjusted + (neutral_value - a_adjusted) * adjustment_factor
        a_adjusted = np.clip(a_adjusted, 0, 255).astype(np.uint8)
        
        # åˆå¹¶é€šé“å¹¶è½¬æ¢å›BGR
        enhanced_lab = cv2.merge([l_enhanced, a_adjusted, b])
        result = cv2.cvtColor(enhanced_lab, cv2.COLOR_LAB2BGR)
        
        # æ­¥éª¤3: è½»å¾®é«˜æ–¯æ¨¡ç³Šï¼ˆå¹³æ»‘è‚Œç†ï¼‰
        if region_type == "core":
            result = cv2.bilateralFilter(result, 5, 40, 40)
        
        return result
    
    def pose_adaptive_processing(self, image, landmarks, intensity, face_pose):
        """
        å§¿æ€è‡ªé€‚åº”å¤„ç† - ä¿æŒåŸå§‹é€»è¾‘ï¼Œé¿å…å¼•å…¥é»‘çº¿é—®é¢˜
        """
        # åŸºäºå§¿æ€è°ƒæ•´å·¦å³åŒºåŸŸå¼ºåº¦
        left_multiplier = 1.0  
        right_multiplier = 1.0
        
        if face_pose == "left_profile":
            left_multiplier = 1.3   # å¯è§ä¾§åŠ å¼º
            right_multiplier = 0.7  # éšè—ä¾§å‡å¼±
        elif face_pose == "right_profile":
            left_multiplier = 0.7   # éšè—ä¾§å‡å¼±  
            right_multiplier = 1.3  # å¯è§ä¾§åŠ å¼º
        
        # ä¿æŒåŸå§‹çš„ç®€å•åˆ†åŒºé€»è¾‘ï¼Œé¿å…å¤æ‚åŒ–
        h, w = image.shape[:2]
        face_center_x = landmarks[1][0]  # é¼»å°–xåæ ‡
        
        left_region_mask = np.zeros((h, w), dtype=np.uint8)
        left_region_mask[:, :face_center_x+2] = 255  # ä¿®å¤ï¼šæ·»åŠ 2åƒç´ é‡å é¿å…é—´éš™
        
        right_region_mask = np.zeros((h, w), dtype=np.uint8) 
        right_region_mask[:, face_center_x-2:] = 255  # ä¿®å¤ï¼šæ·»åŠ 2åƒç´ é‡å é¿å…é—´éš™
        
        return left_multiplier, right_multiplier, left_region_mask, right_region_mask
    
    def face_whitening(self, image, intensity=0.5, region="both", debug=False):
        """ä¸»å¤„ç†å‡½æ•° - è¿›é˜¶åˆ†å±‚ç¾ç™½"""
        landmarks = self.detect_landmarks(image)
        if landmarks is None:
            print("é”™è¯¯ï¼šæœªæ£€æµ‹åˆ°é¢éƒ¨å…³é”®ç‚¹")
            return image
        
        print(f"æ£€æµ‹åˆ° {len(landmarks)} ä¸ªå…³é”®ç‚¹, åŒºåŸŸ: {region}")
        
        # è°ƒè¯•æ¨¡å¼ï¼šæ˜¾ç¤ºæ§åˆ¶ç‚¹å’ŒåŒºåŸŸ
        if debug:
            debug_img = image.copy()
            
            # ç»˜åˆ¶æ ¸å¿ƒç¾ç™½åŒºåŸŸï¼ˆçº¢è‰²ï¼‰
            for idx in self.CORE_WHITENING_REGION:
                if idx < len(landmarks):
                    cv2.circle(debug_img, tuple(landmarks[idx]), 4, (0, 0, 255), -1)
            
            # ç»˜åˆ¶å®Œæ•´é¢éƒ¨è½®å»“ï¼ˆç»¿è‰²ï¼‰
            for idx in self.FULL_FACE_CONTOUR:
                if idx < len(landmarks):
                    cv2.circle(debug_img, tuple(landmarks[idx]), 2, (0, 255, 0), -1)
            
            # æ˜¾ç¤ºå§¿æ€å’Œäº®åº¦ä¿¡æ¯
            pose = self.estimate_face_pose(landmarks)
            brightness = self.analyze_skin_brightness(image, landmarks)
            
            cv2.putText(debug_img, f"Pose: {pose}", (10, 30), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 0), 2)
            cv2.putText(debug_img, f"Brightness: {brightness:.1f}", (10, 60), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 0), 2)
            cv2.putText(debug_img, "Minimal Fix: Convex Hull + Gamma", (10, 90), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
            
            return debug_img
        
        # åˆ†æé¢éƒ¨å§¿æ€å’Œäº®åº¦
        face_pose = self.estimate_face_pose(landmarks)
        brightness_level = self.analyze_skin_brightness(image, landmarks)
        
        print(f"å§¿æ€: {face_pose}, å¹³å‡äº®åº¦: {brightness_level:.1f}")
        
        # åˆ›å»ºåˆ†å±‚é®ç½©
        core_mask, outer_mask = self.create_layered_masks(image, landmarks)
        
        if core_mask.sum() == 0:
            print("è­¦å‘Šï¼šæœªèƒ½åˆ›å»ºæœ‰æ•ˆçš„ç¾ç™½é®ç½©")
            return image
        
        # å§¿æ€è‡ªé€‚åº”å¤„ç†
        left_mult, right_mult, left_mask, right_mask = self.pose_adaptive_processing(
            image, landmarks, intensity, face_pose
        )
        
        print(f"å·¦ä¾§å¼ºåº¦ç³»æ•°: {left_mult:.1f}, å³ä¾§å¼ºåº¦ç³»æ•°: {right_mult:.1f}")
        
        # å¼€å§‹åˆ†å±‚ç¾ç™½å¤„ç†
        result = image.astype(np.float32)
        
        # å¤„ç†æ ¸å¿ƒåŒºåŸŸï¼ˆå¼ºåº¦100%ï¼‰
        if region in ["both", "core"]:
            # å·¦å³åˆ†åŒºå¤„ç†
            if region != "right":  # å¤„ç†å·¦ä¾§
                left_core_mask = cv2.bitwise_and(core_mask, left_mask)
                if left_core_mask.sum() > 0:
                    left_core_region = cv2.bitwise_and(image, image, mask=left_core_mask)
                    enhanced_left_core = self.adaptive_whitening_pipeline(
                        left_core_region, brightness_level, intensity * left_mult, "core"
                    )
                    left_core_mask_norm = left_core_mask.astype(np.float32) / 255.0
                    result = result * (1 - left_core_mask_norm[..., None]) + \
                            enhanced_left_core.astype(np.float32) * left_core_mask_norm[..., None]
            
            if region != "left":  # å¤„ç†å³ä¾§
                right_core_mask = cv2.bitwise_and(core_mask, right_mask)
                if right_core_mask.sum() > 0:
                    right_core_region = cv2.bitwise_and(image, image, mask=right_core_mask)
                    enhanced_right_core = self.adaptive_whitening_pipeline(
                        right_core_region, brightness_level, intensity * right_mult, "core"
                    )
                    right_core_mask_norm = right_core_mask.astype(np.float32) / 255.0
                    result = result * (1 - right_core_mask_norm[..., None]) + \
                            enhanced_right_core.astype(np.float32) * right_core_mask_norm[..., None]
        
        # å¤„ç†å¤–å›´åŒºåŸŸï¼ˆå¼ºåº¦60-80%ï¼‰
        if region in ["both", "outer"]:
            # å·¦å³åˆ†åŒºå¤„ç†å¤–å›´
            if region != "right":  # å¤„ç†å·¦ä¾§
                left_outer_mask = cv2.bitwise_and(outer_mask, left_mask)
                if left_outer_mask.sum() > 0:
                    left_outer_region = cv2.bitwise_and(image, image, mask=left_outer_mask)
                    enhanced_left_outer = self.adaptive_whitening_pipeline(
                        left_outer_region, brightness_level, intensity * left_mult, "outer"
                    )
                    left_outer_mask_norm = left_outer_mask.astype(np.float32) / 255.0
                    result = result * (1 - left_outer_mask_norm[..., None]) + \
                            enhanced_left_outer.astype(np.float32) * left_outer_mask_norm[..., None]
            
            if region != "left":  # å¤„ç†å³ä¾§
                right_outer_mask = cv2.bitwise_and(outer_mask, right_mask)
                if right_outer_mask.sum() > 0:
                    right_outer_region = cv2.bitwise_and(image, image, mask=right_outer_mask)
                    enhanced_right_outer = self.adaptive_whitening_pipeline(
                        right_outer_region, brightness_level, intensity * right_mult, "outer"
                    )
                    right_outer_mask_norm = right_outer_mask.astype(np.float32) / 255.0
                    result = result * (1 - right_outer_mask_norm[..., None]) + \
                            enhanced_right_outer.astype(np.float32) * right_outer_mask_norm[..., None]
        
        # è½¬æ¢å›uint8å¹¶è¿›è¡Œæœ€ç»ˆçš„è¾¹ç•Œå¹³æ»‘
        result = np.clip(result, 0, 255).astype(np.uint8)
        result = cv2.bilateralFilter(result, 3, 20, 20)
        
        return result

def main():
    """æœ€å°åŒ–ä¿®å¤ç‰ˆä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='é¢éƒ¨ç¾ç™½å·¥å…· - åŸºäºåŸå§‹è„šæœ¬çš„æœ€å°åŒ–ä¿®å¤')
    parser.add_argument('--input', '-i', required=True, help='è¾“å…¥å›¾ç‰‡è·¯å¾„')
    parser.add_argument('--output', '-o', required=True, help='è¾“å‡ºå›¾ç‰‡è·¯å¾„')
    parser.add_argument('--intensity', type=float, default=0.5, help='ç¾ç™½å¼ºåº¦(0.3-0.8æ¨è)')
    parser.add_argument('--region', choices=['left', 'right', 'both', 'core', 'outer'], default='both', 
                       help='ç¾ç™½åŒºåŸŸï¼šleft=ä»…å·¦è„¸, right=ä»…å³è„¸, core=ä»…æ ¸å¿ƒåŒºåŸŸ, outer=ä»…å¤–å›´åŒºåŸŸ, both=åˆ†å±‚å¤„ç†')
    parser.add_argument('--debug', action='store_true', help='æ˜¾ç¤ºæ§åˆ¶ç‚¹å’ŒåŒºåŸŸåˆ’åˆ†')
    parser.add_argument('--comparison', action='store_true', help='ç”Ÿæˆå¯¹æ¯”å›¾')
    
    args = parser.parse_args()
    
    # åŠ è½½å›¾ç‰‡
    image = cv2.imread(args.input)
    if image is None:
        print(f"é”™è¯¯ï¼šæ— æ³•åŠ è½½å›¾ç‰‡ {args.input}")
        return
    
    print(f"åŸå›¾å°ºå¯¸: {image.shape}")
    
    # å¼ºåº¦å®‰å…¨æ£€æŸ¥
    if args.intensity > 0.8:
        print(f"è­¦å‘Šï¼šå¼ºåº¦ {args.intensity} å¯èƒ½é€ æˆè¿‡åº¦ç¾ç™½ï¼Œè‡ªåŠ¨é™åˆ¶åˆ°0.8")
        args.intensity = 0.8
    elif args.intensity < 0.1:
        print(f"è­¦å‘Šï¼šå¼ºåº¦ {args.intensity} è¿‡ä½ï¼Œè‡ªåŠ¨è°ƒæ•´åˆ°0.1")
        args.intensity = 0.1
    
    # åˆ›å»ºå¤„ç†å™¨
    processor = AdvancedFaceWhitening()
    
    print("å¼€å§‹æœ€å°åŒ–ä¿®å¤ç‰ˆç¾ç™½å¤„ç†...")
    result = processor.face_whitening(image, args.intensity, args.region, args.debug)
    
    # ä¿å­˜ç»“æœ
    cv2.imwrite(args.output, result)
    print(f"å¤„ç†å®Œæˆï¼ä¿å­˜åˆ°: {args.output}")
    
    # ç”Ÿæˆå¯¹æ¯”å›¾
    if args.comparison and not args.debug:
        comparison = np.hstack([image, result])
        cv2.line(comparison, (image.shape[1], 0), (image.shape[1], image.shape[0]), (0, 255, 0), 3)
        
        # æ·»åŠ æ ‡ç­¾
        cv2.putText(comparison, 'Original', (20, 50), 
                   cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 3)
        cv2.putText(comparison, f'MinimalFix-{args.region.upper()} {args.intensity}', (image.shape[1] + 20, 50), 
                   cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 3)
        
        comparison_path = args.output.replace('.jpg', '_comparison.png').replace('.jpeg', '_comparison.png').replace('.png', '_comparison.png')
        cv2.imwrite(comparison_path, comparison)
        print(f"å¯¹æ¯”å›¾ä¿å­˜åˆ°: {comparison_path}")
    
    # ä¿®å¤è¯´æ˜
    print("\nğŸ”§ æœ€å°åŒ–ä¿®å¤å†…å®¹:")
    print("âœ… å‡¸åŒ…å¤„ç† - ç¡®ä¿é®ç½©åŒºåŸŸè¿ç»­")
    print("âœ… Gammaæ ¡æ­£ - æ›¿ä»£çº¿æ€§å¤„ç†ï¼Œæ›´è‡ªç„¶")
    print("âœ… é‡å åŒºåŸŸ - å·¦å³åˆ†å‰²æ·»åŠ 2åƒç´ é‡å é¿å…é»‘çº¿")
    print("âœ… æ¸©å’Œè°ƒæ•´ - é™ä½aé€šé“è°ƒæ•´å¼ºåº¦")
    print("âœ… PNGæ ¼å¼ - é»˜è®¤ä¿å­˜ä¸ºPNGï¼Œæ›´å¥½çš„ä¿çœŸåº¦")
    print("\nğŸ“– ä¿æŒåŸå§‹é€»è¾‘ï¼Œé¿å…è¿‡åº¦å¤æ‚åŒ–")
    print("ğŸ’¡ åŸºäºæ‚¨çš„åŸå§‹è„šæœ¬ï¼Œåªåšå¿…è¦ä¿®å¤ï¼")
    print("ğŸ–¼ï¸  å»ºè®®ä½¿ç”¨PNGæ ¼å¼è¾“å‡ºä»¥è·å¾—æœ€ä½³è´¨é‡")

if __name__ == "__main__":
    main()
